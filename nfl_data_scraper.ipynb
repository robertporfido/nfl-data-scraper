{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a2d294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import requests\n",
    "import psycopg2\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afdd6ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c66f1143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automate the execution of this to run every day until Feb 20th, 2024 when the season has finished\n",
    "# After 2023 season has ended change the year_list variable below to be 2024\n",
    "\n",
    "year_list = [2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "748e900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Drop all records from 2023 season in SQL database (both raw, dbo)\n",
    "# Step 2: Gather list of game ids from GAMECAST\n",
    "# Step 3: Gather list of game ids from BOXSCORE\n",
    "# Step 4: MODIFIED LIST = GAMECAST LIST - BOXSCORE LIST\n",
    "# Step 5: Insert full data for game ids in BOXSCORE LIST\n",
    "# Step 6: Insert partial data for game ids in MODIFIED LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f4d7a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve postgres database credentials from json file\n",
    "with open('nfl_project_postgres_info.json', 'r') as file:\n",
    "    # Read the file contents\n",
    "    json_data = file.read()\n",
    "\n",
    "    # Parse the JSON data\n",
    "    postgres_info = json.loads(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378719cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    \n",
    "        host=\"localhost\",\n",
    "        database=postgres_info['database_name'],\n",
    "        user=postgres_info['nfl_project_username'],\n",
    "        password=postgres_info['nfl_project_password']\n",
    ")\n",
    "\n",
    "# Create a cursor\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Prepare the SQL statement to delete records\n",
    "sql = \"DELETE FROM raw.nfl_game_data WHERE season = 2023\"\n",
    "sql2 = \"DELETE FROM dbo.nfl_game_data WHERE season = 2023\"\n",
    "\n",
    "# Execute the delete statement\n",
    "cursor.execute(sql)\n",
    "cursor.execute(sql2)\n",
    "\n",
    "# Commit the changes to the database\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c71db0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESPN NFL Schedule URL\n",
    "base_url = \"https://www.espn.com/nfl/scoreboard/_/week/{}/year/{}/seasontype/{}\"\n",
    "season_type = [2,3]  # 2 for regular season, 3 for playoffs\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Create a game_id list\n",
    "gamecast_game_ids = []\n",
    "\n",
    "for year in year_list:\n",
    "    for season in season_type:\n",
    "        for week in range(1, 20):  # Regular season has 17 weeks\n",
    "            url = base_url.format(week, year, season)\n",
    "            driver.get(url)\n",
    "\n",
    "            # Get the HTML content after the page is loaded\n",
    "            html_content = driver.page_source\n",
    "\n",
    "            # Create a BeautifulSoup object from the HTML content\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "            # Find all anchor tags (a) with href attributes containing \"gameId/\"\n",
    "            anchor_tags = soup.find_all('a', href=lambda href: href and 'game/_/gameId/' in href)\n",
    "\n",
    "            # Extract the href attribute values\n",
    "            hrefs = [tag['href'] for tag in anchor_tags]\n",
    "            \n",
    "            # Retrieve week information\n",
    "            week = soup.find_all(\"div\", class_=\"custom--week is-active\")\n",
    "            try:\n",
    "                week_element = week[0]\n",
    "                week_span = week_element.find('span', class_='week week-range')\n",
    "                week_number = week_span.text.strip()\n",
    "            except IndexError: week_number = ''\n",
    "\n",
    "            for href in hrefs:\n",
    "                start_index = href.find(\"gameId/\") + len(\"gameId/\")  # Find the index after \"gameId/\"\n",
    "                extracted_string = href[start_index:start_index + 9]  # Extract the 9-digit gameID\n",
    "                gamecast_game_ids.append((extracted_string,season,week_number,year))\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70bba0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESPN NFL Schedule URL\n",
    "base_url = \"https://www.espn.com/nfl/scoreboard/_/week/{}/year/{}/seasontype/{}\"\n",
    "season_type = [2,3]  # 2 for regular season, 3 for playoffs\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Create a game_id list\n",
    "boxscore_game_ids = []\n",
    "\n",
    "for year in year_list:\n",
    "    for season in season_type:\n",
    "        for week in range(1, 20):  # Regular season has 17 weeks\n",
    "            url = base_url.format(week, year, season)\n",
    "            driver.get(url)\n",
    "\n",
    "            # Get the HTML content after the page is loaded\n",
    "            html_content = driver.page_source\n",
    "\n",
    "            # Create a BeautifulSoup object from the HTML content\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "            # Find all anchor tags (a) with href attributes containing \"gameId/\"\n",
    "            anchor_tags = soup.find_all('a', href=lambda href: href and 'boxscore/_/gameId/' in href)\n",
    "\n",
    "            # Extract the href attribute values\n",
    "            hrefs = [tag['href'] for tag in anchor_tags]\n",
    "            \n",
    "            # Retrieve week information\n",
    "            week = soup.find_all(\"div\", class_=\"custom--week is-active\")\n",
    "            try:\n",
    "                week_element = week[0]\n",
    "                week_span = week_element.find('span', class_='week week-range')\n",
    "                week_number = week_span.text.strip()\n",
    "            except IndexError: week_number = ''\n",
    "\n",
    "            for href in hrefs:\n",
    "                start_index = href.find(\"gameId/\") + len(\"gameId/\")  # Find the index after \"gameId/\"\n",
    "                extracted_string = href[start_index:start_index + 9]  # Extract the 9-digit gameID\n",
    "                boxscore_game_ids.append((extracted_string,season,week_number,year))\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1accfc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the set of game_ids\n",
    "modified_game_id_list = [tup for tup in gamecast_game_ids if not any(tup[0] == item[0] for item in boxscore_game_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f929c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game data list of dictionaries\n",
    "game_data = []\n",
    "\n",
    "# Initial URL\n",
    "base_url = \"https://www.espn.com/nfl/boxscore/_/gameId/{}\"\n",
    "\n",
    "for game_id in boxscore_game_ids:\n",
    "    \n",
    "    # Print for troubleshooting visibility\n",
    "    # print(game_id)\n",
    "    \n",
    "    url = base_url.format(game_id[0])\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    if game_id[1] == 2:\n",
    "        part_of_season = 'regular_season'\n",
    "    elif game_id[1] == 3:\n",
    "        part_of_season = 'playoffs'\n",
    "    \n",
    "    team_name_elements = soup.find_all(\"div\", class_=\"Gamestrip__InfoLogo\")\n",
    "    \n",
    "    try:\n",
    "        a_element = team_name_elements[0].find('a')\n",
    "        parts = a_element['href'].split('/')\n",
    "        away_team = parts[-1].replace('-', '_')\n",
    "    except AttributeError: away_team = ''\n",
    "        \n",
    "    try:\n",
    "        a_element = team_name_elements[1].find('a')\n",
    "        parts = a_element['href'].split('/')\n",
    "        home_team = parts[-1].replace('-', '_')\n",
    "    except AttributeError: home_team = ''\n",
    "    \n",
    "    try:\n",
    "        game_location = soup.find(\"span\", class_=\"Location__Text\").text.strip()\n",
    "    except AttributeError: game_location = ''\n",
    "        \n",
    "    try:\n",
    "        game_stadium = soup.find(\"div\", class_=\"n6 clr-gray-03 GameInfo__Location__Name\").text.strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            game_stadium = soup.find(\"div\", class_=\"n6 clr-gray-03 GameInfo__Location__Name--noImg\").text.strip()\n",
    "        except AttributeError: game_stadium = ''\n",
    "    \n",
    "    game_datetime_string = soup.find(\"div\", class_=\"n8 GameInfo__Meta\").text.strip()\n",
    "    try:\n",
    "        game_time = game_datetime_string.split(',', 1)[0]\n",
    "    except AttributeError: game_time = ''\n",
    "    \n",
    "    try:\n",
    "        game_date = game_datetime_string.split(',', 1)[1].split('Coverage:', 1)[0].strip()\n",
    "    except AttributeError: game_date = ''\n",
    "        \n",
    "    try:\n",
    "        attendance = soup.find(\"div\", class_=\"Attendance__Numbers\").text.strip()\n",
    "        digits = ''.join(filter(str.isdigit, attendance))\n",
    "        game_attendance = int(digits)\n",
    "    except AttributeError: game_attendance = ''\n",
    "    \n",
    "    try:\n",
    "        capacity = soup.find(\"div\", class_=\"Attendance__Capacity h10\").text.strip()\n",
    "        digits = ''.join(filter(str.isdigit, capacity))\n",
    "        stadium_capacity = int(digits)\n",
    "    except AttributeError: stadium_capacity = ''\n",
    "    \n",
    "    try:\n",
    "        percent = soup.find(\"div\", class_=\"n3 flex-expand Attendance__Percentage\").text.strip()\n",
    "        digits = ''.join(filter(str.isdigit, percent))\n",
    "        attendance_percent = int(digits)\n",
    "    except AttributeError: attendance_percent = ''\n",
    "        \n",
    "    team_records = soup.find_all(\"div\", class_=\"Gamestrip__Record db n10 clr-gray-03\")\n",
    "    away_team_total_record = team_records[0].text.split()[0].strip(',')\n",
    "    away_team_away_record = team_records[0].text.split()[1]\n",
    "    home_team_total_record = team_records[0].text.split()[0].strip(',')\n",
    "    home_team_home_record = team_records[0].text.split()[1]\n",
    "    \n",
    "    team_scores = soup.find_all(\"tr\", class_=\"Table__TR Table__TR--sm Table__even\")\n",
    "    \n",
    "    if team_scores[0].find_all()[1].text.isdigit() == True:\n",
    "        away_team_quarter_one = int(team_scores[0].find_all()[1].text)\n",
    "        away_team_quarter_second = int(team_scores[0].find_all()[2].text)\n",
    "        away_team_quarter_third = int(team_scores[0].find_all()[3].text)\n",
    "        away_team_quarter_fourth = int(team_scores[0].find_all()[4].text)\n",
    "        if len(team_scores[0].find_all()) == 7:\n",
    "            away_team_quarter_ot = int(team_scores[0].find_all()[5].text)\n",
    "            away_team_final = int(team_scores[0].find_all()[6].text)\n",
    "        else:\n",
    "            away_team_quarter_ot = np.nan\n",
    "            away_team_final = int(team_scores[0].find_all()[5].text)\n",
    "\n",
    "        home_team_quarter_one = int(team_scores[1].find_all()[1].text)\n",
    "        home_team_quarter_second = int(team_scores[1].find_all()[2].text)\n",
    "        home_team_quarter_third = int(team_scores[1].find_all()[3].text)\n",
    "        home_team_quarter_fourth = int(team_scores[1].find_all()[4].text)\n",
    "        if len(team_scores[0].find_all()) == 7:\n",
    "            home_team_quarter_ot = int(team_scores[1].find_all()[5].text)\n",
    "            home_team_final = int(team_scores[1].find_all()[6].text)\n",
    "        else:\n",
    "            home_team_quarter_ot = np.nan\n",
    "            home_team_final = int(team_scores[1].find_all()[5].text)\n",
    "    else:\n",
    "        away_team_quarter_one = np.nan\n",
    "        away_team_quarter_second = np.nan\n",
    "        away_team_quarter_third = np.nan\n",
    "        away_team_quarter_fourth = np.nan\n",
    "        away_team_quarter_ot = np.nan\n",
    "        away_team_final = np.nan\n",
    "        home_team_quarter_one = np.nan\n",
    "        home_team_quarter_second = np.nan\n",
    "        home_team_quarter_third = np.nan\n",
    "        home_team_quarter_fourth = np.nan\n",
    "        home_team_quarter_ot = np.nan\n",
    "        home_team_final = np.nan\n",
    "        \n",
    "    team_stats = soup.find_all(\"tr\", class_=\"Boxscore__Totals Table__TR Table__TR--sm Table__even\")\n",
    "    \n",
    "    if len(team_stats) > 0:\n",
    "    \n",
    "        away_pass_comp_attempts = team_stats[1].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[0].text\n",
    "        away_pass_yards = team_stats[1].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[1].text\n",
    "        away_pass_tds = team_stats[1].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[3].text\n",
    "        away_pass_ints = team_stats[1].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[4].text\n",
    "        away_sacks_allowed = team_stats[1].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[5].text\n",
    "        if team_stats[1].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[6].text == '--':\n",
    "            away_pass_rating = team_stats[1].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[7].text\n",
    "        else: away_pass_rating = team_stats[1].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[6].text\n",
    "        away_rush_attempts = team_stats[5].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[0].text\n",
    "        away_rush_yards = team_stats[5].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[1].text\n",
    "        away_rush_tds = team_stats[5].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[3].text\n",
    "        away_rush_long = team_stats[5].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[4].text\n",
    "        away_rec_targets = team_stats[9].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[5].text\n",
    "        away_rec_receptions = team_stats[9].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[0].text\n",
    "        away_rec_yards = team_stats[9].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[1].text\n",
    "        away_rec_tds = team_stats[9].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[3].text\n",
    "        away_rec_long = team_stats[9].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[4].text\n",
    "\n",
    "        home_pass_comp_attempts = team_stats[3].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[0].text\n",
    "        home_pass_yards = team_stats[3].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[1].text\n",
    "        home_pass_tds = team_stats[3].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[3].text\n",
    "        home_pass_ints = team_stats[3].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[4].text\n",
    "        home_sacks_allowed = team_stats[3].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[5].text\n",
    "        if team_stats[3].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[6].text == '--':\n",
    "            home_pass_rating = team_stats[3].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[7].text\n",
    "        else: home_pass_rating = team_stats[3].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[6].text\n",
    "        home_rush_attempts = team_stats[7].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[0].text\n",
    "        home_rush_yards = team_stats[7].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[1].text\n",
    "        home_rush_tds = team_stats[7].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[3].text\n",
    "        home_rush_long = team_stats[7].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[4].text\n",
    "        home_rec_targets = team_stats[11].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[5].text\n",
    "        home_rec_receptions = team_stats[11].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[0].text\n",
    "        home_rec_yards = team_stats[11].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[1].text\n",
    "        home_rec_tds = team_stats[11].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[3].text\n",
    "        home_rec_long = team_stats[11].find_all(\"td\", class_=\"Boxscore__Totals_Items Table__TD\")[4].text\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        away_pass_comp_attempts = np.nan\n",
    "        away_pass_yards = np.nan\n",
    "        away_pass_tds = np.nan\n",
    "        away_pass_ints = np.nan\n",
    "        away_sacks_allowed = np.nan\n",
    "        away_pass_rating = np.nan\n",
    "        away_rush_attempts = np.nan\n",
    "        away_rush_yards = np.nan\n",
    "        away_rush_tds = np.nan\n",
    "        away_rush_long = np.nan\n",
    "        away_rec_targets = np.nan\n",
    "        away_rec_receptions = np.nan\n",
    "        away_rec_yards = np.nan\n",
    "        away_rec_tds = np.nan\n",
    "        away_rec_long = np.nan\n",
    "\n",
    "        home_pass_comp_attempts = np.nan\n",
    "        home_pass_yards = np.nan\n",
    "        home_pass_tds = np.nan\n",
    "        home_pass_ints = np.nan\n",
    "        home_sacks_allowed = np.nan\n",
    "        home_pass_rating = np.nan\n",
    "        home_rush_attempts = np.nan\n",
    "        home_rush_yards = np.nan\n",
    "        home_rush_tds = np.nan\n",
    "        home_rush_long = np.nan\n",
    "        home_rec_targets = np.nan\n",
    "        home_rec_receptions = np.nan\n",
    "        home_rec_yards = np.nan\n",
    "        home_rec_tds = np.nan\n",
    "        home_rec_long = np.nan\n",
    "    \n",
    "    game_data.append(\n",
    "    \n",
    "    {'espn_game_id':game_id[0],\n",
    "     'season':game_id[3],\n",
    "     'part_of_season':part_of_season,\n",
    "     'week_number':game_id[2],\n",
    "     'away_team':away_team,\n",
    "     'home_team':home_team,\n",
    "     'game_location':game_location,\n",
    "     'game_stadium':game_stadium,\n",
    "     'game_time':game_time,\n",
    "     'game_date':game_date,\n",
    "     'game_attendance':game_attendance,\n",
    "     'stadium_capacity':stadium_capacity,\n",
    "     'attendance_percent':attendance_percent,\n",
    "     'weather':'',\n",
    "     'away_team_total_record':away_team_total_record,\n",
    "     'away_team_away_record':away_team_away_record,\n",
    "     'home_team_total_record':home_team_total_record,\n",
    "     'home_team_home_record':home_team_home_record,\n",
    "     'away_team_quarter_first':away_team_quarter_one,\n",
    "     'away_team_quarter_second':away_team_quarter_second,\n",
    "     'away_team_quarter_third':away_team_quarter_third,\n",
    "     'away_team_quarter_fourth':away_team_quarter_fourth,\n",
    "     'away_team_quarter_ot':away_team_quarter_ot,\n",
    "     'away_team_final':away_team_final,\n",
    "     'away_pass_comp_attempts':away_pass_comp_attempts,\n",
    "     'away_pass_yards':away_pass_yards,\n",
    "     'away_pass_tds':away_pass_tds,\n",
    "     'away_pass_ints':away_pass_ints,\n",
    "     'away_sacks_allowed':away_sacks_allowed,\n",
    "     'away_pass_rating':away_pass_rating,\n",
    "     'away_rush_attempts':away_rush_attempts,\n",
    "     'away_rush_yards':away_rush_yards,\n",
    "     'away_rush_tds':away_rush_tds,\n",
    "     'away_rush_long':away_rush_long,\n",
    "     'away_rec_targets':away_rec_targets,\n",
    "     'away_rec_receptions':away_rec_receptions,\n",
    "     'away_rec_yards':away_rec_yards,\n",
    "     'away_rec_tds':away_rec_tds,\n",
    "     'away_rec_long':away_rec_long,\n",
    "     'home_team_quarter_first':home_team_quarter_one,\n",
    "     'home_team_quarter_second':home_team_quarter_second,\n",
    "     'home_team_quarter_third':home_team_quarter_third,\n",
    "     'home_team_quarter_fourth':home_team_quarter_fourth,\n",
    "     'home_team_quarter_ot':home_team_quarter_ot,\n",
    "     'home_team_final':home_team_final,\n",
    "     'home_pass_comp_attempts':home_pass_comp_attempts,\n",
    "     'home_pass_yards':home_pass_yards,\n",
    "     'home_pass_tds':home_pass_tds,\n",
    "     'home_pass_ints':home_pass_ints,\n",
    "     'home_sacks_allowed':home_sacks_allowed,\n",
    "     'home_pass_rating':home_pass_rating,\n",
    "     'home_rush_attempts':home_rush_attempts,\n",
    "     'home_rush_yards':home_rush_yards,\n",
    "     'home_rush_tds':home_rush_tds,\n",
    "     'home_rush_long':home_rush_long,\n",
    "     'home_rec_targets':home_rec_targets,\n",
    "     'home_rec_receptions':home_rec_receptions,\n",
    "     'home_rec_yards':home_rec_yards,\n",
    "     'home_rec_tds':home_rec_tds,\n",
    "     'home_rec_long':home_rec_long,\n",
    "     \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df652ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary to dataframe\n",
    "game_table = pd.DataFrame(game_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e5c348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    \n",
    "        host=\"localhost\",\n",
    "        database=postgres_info['database_name'],\n",
    "        user=postgres_info['nfl_project_username'],\n",
    "        password=postgres_info['nfl_project_password']\n",
    ")\n",
    "\n",
    "# Create a cursor\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Convert the DataFrame to a list of tuples\n",
    "data = [tuple(row) for row in game_table.values]\n",
    "\n",
    "# Get the column names from the DataFrame\n",
    "columns = list(game_table.columns)\n",
    "\n",
    "# Prepare the SQL statement for batch insertion\n",
    "sql = \"INSERT INTO raw.nfl_game_data ({}) VALUES %s\".format(\", \".join(columns))  # Dynamically generate column names\n",
    "\n",
    "from psycopg2 import extras\n",
    "\n",
    "# Execute the batch insert\n",
    "extras.execute_values(cursor, sql, data)\n",
    "\n",
    "# Commit the changes to the database\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2230b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game data list of dictionaries\n",
    "pre_game_data = []\n",
    "\n",
    "# Initial URL\n",
    "base_url = \"https://www.espn.com/nfl/game/_/gameId/{}\"\n",
    "\n",
    "for game_id in modified_game_id_list:\n",
    "    \n",
    "    # Print for troubleshooting visibility\n",
    "    # print(game_id)\n",
    "    \n",
    "    url = base_url.format(game_id[0])\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    if game_id[1] == 2:\n",
    "        part_of_season = 'regular_season'\n",
    "    elif game_id[1] == 3:\n",
    "        part_of_season = 'playoffs'\n",
    "    \n",
    "    team_name_elements = soup.find_all(\"div\", class_=\"Gamestrip__InfoLogo\")\n",
    "    \n",
    "    try:\n",
    "        a_element = team_name_elements[0].find('a')\n",
    "        parts = a_element['href'].split('/')\n",
    "        away_team = parts[-1].replace('-', '_')\n",
    "    except AttributeError: away_team = ''\n",
    "    except TypeError: away_team = ''\n",
    "        \n",
    "    try:\n",
    "        a_element = team_name_elements[1].find('a')\n",
    "        parts = a_element['href'].split('/')\n",
    "        home_team = parts[-1].replace('-', '_')\n",
    "    except AttributeError: home_team = ''\n",
    "    except TypeError: home_team = ''\n",
    "    \n",
    "    try:\n",
    "        game_location = soup.find(\"span\", class_=\"Location__Text\").text.strip()\n",
    "    except AttributeError: game_location = ''\n",
    "    except TypeError: game_location = ''\n",
    "        \n",
    "    try:\n",
    "        game_stadium = soup.find(\"div\", class_=\"n6 clr-gray-03 GameInfo__Location__Name\").text.strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            game_stadium = soup.find(\"div\", class_=\"n6 clr-gray-03 GameInfo__Location__Name--noImg\").text.strip()\n",
    "        except AttributeError: game_stadium = ''\n",
    "        except TypeError: game_stadium = ''\n",
    "    except TypeError: game_stadium = ''\n",
    "    \n",
    "    game_datetime_string = soup.find(\"div\", class_=\"n8 GameInfo__Meta\").text.strip()\n",
    "    try:\n",
    "        game_time = game_datetime_string.split(',', 1)[0]\n",
    "    except AttributeError: game_time = ''\n",
    "    except TypeError: game_time = ''\n",
    "    \n",
    "    try:\n",
    "        game_date = game_datetime_string.split(',', 1)[1].split('Coverage:', 1)[0].strip()\n",
    "    except AttributeError: game_date = ''\n",
    "    except TypeError: game_date = ''\n",
    "        \n",
    "    game_attendance = np.nan\n",
    "    \n",
    "    try:\n",
    "        capacity = soup.find(\"div\", class_=\"Attendance__Capacity h10\").text.strip()\n",
    "        digits = ''.join(filter(str.isdigit, capacity))\n",
    "        stadium_capacity = int(digits)\n",
    "    except AttributeError: stadium_capacity = ''\n",
    "    except TypeError: stadium_capacity = ''\n",
    "    \n",
    "    attendance_percent = np.nan\n",
    "\n",
    "    away_team_total_record = np.nan\n",
    "    away_team_away_record = np.nan\n",
    "    home_team_total_record = np.nan\n",
    "    home_team_home_record = np.nan\n",
    "    \n",
    "    away_team_quarter_one = np.nan\n",
    "    away_team_quarter_second = np.nan\n",
    "    away_team_quarter_third = np.nan\n",
    "    away_team_quarter_fourth = np.nan\n",
    "    away_team_quarter_ot = np.nan\n",
    "    away_team_final = np.nan\n",
    "    home_team_quarter_one = np.nan\n",
    "    home_team_quarter_second = np.nan\n",
    "    home_team_quarter_third = np.nan\n",
    "    home_team_quarter_fourth = np.nan\n",
    "    home_team_quarter_ot = np.nan\n",
    "    home_team_final = np.nan\n",
    "         \n",
    "    away_pass_comp_attempts = np.nan\n",
    "    away_pass_yards = np.nan\n",
    "    away_pass_tds = np.nan\n",
    "    away_pass_ints = np.nan\n",
    "    away_sacks_allowed = np.nan\n",
    "    away_pass_rating = np.nan\n",
    "    away_rush_attempts = np.nan\n",
    "    away_rush_yards = np.nan\n",
    "    away_rush_tds = np.nan\n",
    "    away_rush_long = np.nan\n",
    "    away_rec_targets = np.nan\n",
    "    away_rec_receptions = np.nan\n",
    "    away_rec_yards = np.nan\n",
    "    away_rec_tds = np.nan\n",
    "    away_rec_long = np.nan\n",
    "\n",
    "    home_pass_comp_attempts = np.nan\n",
    "    home_pass_yards = np.nan\n",
    "    home_pass_tds = np.nan\n",
    "    home_pass_ints = np.nan\n",
    "    home_sacks_allowed = np.nan\n",
    "    home_pass_rating = np.nan\n",
    "    home_rush_attempts = np.nan\n",
    "    home_rush_yards = np.nan\n",
    "    home_rush_tds = np.nan\n",
    "    home_rush_long = np.nan\n",
    "    home_rec_targets = np.nan\n",
    "    home_rec_receptions = np.nan\n",
    "    home_rec_yards = np.nan\n",
    "    home_rec_tds = np.nan\n",
    "    home_rec_long = np.nan\n",
    "    \n",
    "    pre_game_data.append(\n",
    "    \n",
    "    {'espn_game_id':game_id[0],\n",
    "     'season':game_id[3],\n",
    "     'part_of_season':part_of_season,\n",
    "     'week_number':game_id[2],\n",
    "     'away_team':away_team,\n",
    "     'home_team':home_team,\n",
    "     'game_location':game_location,\n",
    "     'game_stadium':game_stadium,\n",
    "     'game_time':game_time,\n",
    "     'game_date':game_date,\n",
    "     'game_attendance':game_attendance,\n",
    "     'stadium_capacity':stadium_capacity,\n",
    "     'attendance_percent':attendance_percent,\n",
    "     'weather':'',\n",
    "     'away_team_total_record':away_team_total_record,\n",
    "     'away_team_away_record':away_team_away_record,\n",
    "     'home_team_total_record':home_team_total_record,\n",
    "     'home_team_home_record':home_team_home_record,\n",
    "     'away_team_quarter_first':away_team_quarter_one,\n",
    "     'away_team_quarter_second':away_team_quarter_second,\n",
    "     'away_team_quarter_third':away_team_quarter_third,\n",
    "     'away_team_quarter_fourth':away_team_quarter_fourth,\n",
    "     'away_team_quarter_ot':away_team_quarter_ot,\n",
    "     'away_team_final':away_team_final,\n",
    "     'away_pass_comp_attempts':away_pass_comp_attempts,\n",
    "     'away_pass_yards':away_pass_yards,\n",
    "     'away_pass_tds':away_pass_tds,\n",
    "     'away_pass_ints':away_pass_ints,\n",
    "     'away_sacks_allowed':away_sacks_allowed,\n",
    "     'away_pass_rating':away_pass_rating,\n",
    "     'away_rush_attempts':away_rush_attempts,\n",
    "     'away_rush_yards':away_rush_yards,\n",
    "     'away_rush_tds':away_rush_tds,\n",
    "     'away_rush_long':away_rush_long,\n",
    "     'away_rec_targets':away_rec_targets,\n",
    "     'away_rec_receptions':away_rec_receptions,\n",
    "     'away_rec_yards':away_rec_yards,\n",
    "     'away_rec_tds':away_rec_tds,\n",
    "     'away_rec_long':away_rec_long,\n",
    "     'home_team_quarter_first':home_team_quarter_one,\n",
    "     'home_team_quarter_second':home_team_quarter_second,\n",
    "     'home_team_quarter_third':home_team_quarter_third,\n",
    "     'home_team_quarter_fourth':home_team_quarter_fourth,\n",
    "     'home_team_quarter_ot':home_team_quarter_ot,\n",
    "     'home_team_final':home_team_final,\n",
    "     'home_pass_comp_attempts':home_pass_comp_attempts,\n",
    "     'home_pass_yards':home_pass_yards,\n",
    "     'home_pass_tds':home_pass_tds,\n",
    "     'home_pass_ints':home_pass_ints,\n",
    "     'home_sacks_allowed':home_sacks_allowed,\n",
    "     'home_pass_rating':home_pass_rating,\n",
    "     'home_rush_attempts':home_rush_attempts,\n",
    "     'home_rush_yards':home_rush_yards,\n",
    "     'home_rush_tds':home_rush_tds,\n",
    "     'home_rush_long':home_rush_long,\n",
    "     'home_rec_targets':home_rec_targets,\n",
    "     'home_rec_receptions':home_rec_receptions,\n",
    "     'home_rec_yards':home_rec_yards,\n",
    "     'home_rec_tds':home_rec_tds,\n",
    "     'home_rec_long':home_rec_long,\n",
    "     \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba47c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary to dataframe\n",
    "pre_game_table = pd.DataFrame(pre_game_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccdcac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    \n",
    "        host=\"localhost\",\n",
    "        database=postgres_info['database_name'],\n",
    "        user=postgres_info['nfl_project_username'],\n",
    "        password=postgres_info['nfl_project_password']\n",
    ")\n",
    "\n",
    "# Create a cursor\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Convert the DataFrame to a list of tuples\n",
    "data = [tuple(row) for row in pre_game_table.values]\n",
    "\n",
    "# Get the column names from the DataFrame\n",
    "columns = list(pre_game_table.columns)\n",
    "\n",
    "# Prepare the SQL statement for batch insertion\n",
    "sql = \"INSERT INTO raw.nfl_game_data ({}) VALUES %s\".format(\", \".join(columns))  # Dynamically generate column names\n",
    "\n",
    "from psycopg2 import extras\n",
    "\n",
    "# Execute the batch insert\n",
    "extras.execute_values(cursor, sql, data)\n",
    "\n",
    "# Commit the changes to the database\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
